{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Código Reporte Final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mxJ9Fdlx7Vcq",
        "ahC8C8qW9XAW",
        "9R7nLCHK9kdX"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WEpGGYPQ2yx"
      },
      "source": [
        "# Extracción de Texto con BERT\n",
        "\n",
        "Integrantes del equipo: \n",
        "\n",
        "*   Rodrigo Salinas 124825\n",
        "*   Oscar Ruiz 195238\n",
        "*   Viridiana Marlen González Flores 192460\n",
        "*   Victor Rivera 97105\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXe3Fc-N9GmC"
      },
      "source": [
        "# Código y Comentarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGrTNfU8_gT-"
      },
      "source": [
        "##Configuración"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3JiFr3OW4cG"
      },
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU8G2xVo_lIV"
      },
      "source": [
        "# Parámetros y configuraciones iniciales para BERT\n",
        "import os # el módulo provee funciones para interactuar con el Sistema Operativo.\n",
        "import re # el módulo provee herramientas de expresiones regulares para procesamiento en cadenas.\n",
        "import json # el módulo permite trabajar con archivos JSON. \n",
        "import string # conserva varias constantes útiles y clases para trabajar con objetos str.\n",
        "import numpy as np # Numpy es un paquete que provee a Python con arreglos multidimensionales de alta eficiencia y diseñados para cálculo científico.\n",
        "import tensorflow as tf # TensorFlow facilita la creación de modelos de aprendizaje automático.\n",
        "from tensorflow import keras # soporta la libreria TensorFlow.\n",
        "from tensorflow.keras import layers # se importan las capas de keras.\n",
        "from tokenizers import BertWordPieceTokenizer # tokenización de subpalabras.\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig # configuraciones para BERT. \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "max_len = 384 #Tamaño máximo de capas\n",
        "configuration = BertConfig()  # default parameters and configuration for BERT\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vko1Cd6_3jo"
      },
      "source": [
        "## Configuración del tokenizador BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMTIGL1l_6fG"
      },
      "source": [
        "# Guarda el token preentrenado\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "save_path = \"bert_base_uncased/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Carga el token desde el archivo previamente guardado\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6wzHqZFADWp"
      },
      "source": [
        "##Carga de datos de entrenamiento en inglés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOyt2OzD_H1R"
      },
      "source": [
        "train_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
        "train_path = keras.utils.get_file(\"train.json\", train_data_url)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-tn1ORbBRRd"
      },
      "source": [
        "## Carga de datos de evaluación en español\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--ts5FEOBUUe"
      },
      "source": [
        "all_data_url = \"https://github.com/deepmind/xquad/raw/master/xquad.es.json\" #Carga los datos de entrenamiento en español\n",
        "all_path = keras.utils.get_file(\"train.json\", all_data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZDEFMTNGHP5"
      },
      "source": [
        "## Carga de datos de evaluación en inglés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mbx9GSyGLAQ"
      },
      "source": [
        "eval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\" #Carga los datos de prueba en inglés\n",
        "eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k9GocsuGzdw"
      },
      "source": [
        "Nota: si se quiere evaluar en modelo con los datos de evaluación en inglés, deshabilitar la carga de datos en español y habilitar la carga de datos en inglés, así mismo, hacer el cambio en los artículos en la función normalize_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axTdF1kOARCE"
      },
      "source": [
        "## Preprocesamiento de los datos\n",
        "1. Recorre el archivo JSON y almacena cada registro como un objeto SquadExample.\n",
        "2. Revisa cada SquadExample y \"create x_train\", \"y_train\", \"x_eval\", \"y_eval\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqoXWsQyAWKt"
      },
      "source": [
        "class SquadExample:\n",
        "    def __init__(self, question, context, start_char_idx, answer_text, all_answers):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.all_answers = all_answers\n",
        "        self.skip = False\n",
        "\n",
        "    def preprocess(self):\n",
        "        context = self.context\n",
        "        question = self.question\n",
        "        answer_text = self.answer_text\n",
        "        start_char_idx = self.start_char_idx\n",
        "\n",
        "        # Limpiar contexto, respuesta and pregunta\n",
        "        context = \" \".join(str(context).split())\n",
        "        question = \" \".join(str(question).split())\n",
        "        answer = \" \".join(str(answer_text).split())\n",
        "\n",
        "        # Encontrar el índice de último carácter de la respuesta en el contexto\n",
        "        end_char_idx = start_char_idx + len(answer)\n",
        "        if end_char_idx >= len(context):\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Marcar los índices de los caracteres en el contexto que están en la respuesta\n",
        "        is_char_in_ans = [0] * len(context)\n",
        "        for idx in range(start_char_idx, end_char_idx):\n",
        "            is_char_in_ans[idx] = 1\n",
        "\n",
        "        # Tokenizar contexto\n",
        "        tokenized_context = tokenizer.encode(context)\n",
        "\n",
        "        # Encontrar tokens que fueron creados de los caracteres de la respuesta\n",
        "        ans_token_idx = []\n",
        "        for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
        "            if sum(is_char_in_ans[start:end]) > 0:\n",
        "                ans_token_idx.append(idx)\n",
        "\n",
        "        if len(ans_token_idx) == 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Encontrar los índices de inicio y fin de los tokens en la respuesta\n",
        "        start_token_idx = ans_token_idx[0]\n",
        "        end_token_idx = ans_token_idx[-1]\n",
        "\n",
        "        # Tokenizar pregunta\n",
        "        tokenized_question = tokenizer.encode(question)\n",
        "\n",
        "        # Crear entradas\n",
        "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
        "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n",
        "            tokenized_question.ids[1:]\n",
        "        )\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Rellenar y crear máscaras de atención.\n",
        "        # Saltar si es necesario truncar\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        if padding_length > 0:  # rellenar\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:  # saltar\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.context_token_to_char = tokenized_context.offsets\n",
        "\n",
        "\n",
        "with open(train_path) as f:\n",
        "    raw_train_data = json.load(f)\n",
        "\n",
        "with open(eval_path) as f:\n",
        "    raw_eval_data = json.load(f)\n",
        "\n",
        "\n",
        "def create_squad_examples(raw_data):\n",
        "    squad_examples = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                squad_eg = SquadExample(\n",
        "                    question, context, start_char_idx, answer_text, all_answers\n",
        "                )\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "    return squad_examples\n",
        "\n",
        "\n",
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_squad_examples = create_squad_examples(raw_train_data)\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\n",
        "\n",
        "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj6u1G_aAkfC"
      },
      "source": [
        "##Creación del modelo de respuesta a preguntas mediante BERT y API funcional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RBgN-YnAps4"
      },
      "source": [
        "def create_model():\n",
        "    ## Codificador BERT\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    ## Modelo de preguntas y respuestas\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    embedding = encoder(\n",
        "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
        "    ## Generación de probabilidades\n",
        "    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
        "    start_logits = layers.Flatten()(start_logits)\n",
        "\n",
        "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
        "    end_logits = layers.Flatten()(end_logits)\n",
        "    ## Función de activación de las capas\n",
        "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[start_probs, end_probs],\n",
        "    )\n",
        "    ## Función de pérdida de entropía cruzada \n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    # Esta es la tasa de aprendizaje, según el paper, los valores óptimos son:\n",
        "    # 5e-5, 3r-5, 2e-5\n",
        "    optimizer = keras.optimizers.Adam(lr=5e-5)\n",
        "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls_vEQXAAwD_"
      },
      "source": [
        "##Este código debe ejecutarse preferentemente en el tiempo de ejecución de Google Colab TPU.\n",
        "Con los TPUs de Colab, cada época tardará entre 5 y 6 minutos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g8HvJMKkA2cC"
      },
      "source": [
        "use_tpu = True\n",
        "if use_tpu:\n",
        "    # Se crea una estrategia de distribución\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "    # Creación del modelo\n",
        "    with strategy.scope():\n",
        "        model = create_model()\n",
        "else:\n",
        "    model = create_model()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULwUIYHBA5-4"
      },
      "source": [
        "##Evaluación de \"Callback\"\n",
        "\n",
        "Esta llamada de retorno calculará la puntuación de coincidencia exacta utilizando los datos de validación después de cada época."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c3IQLJxm_Z5Z"
      },
      "source": [
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    # Elimina los signos de puntuación\n",
        "    exclude = set(string.punctuation)\n",
        "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    # Eliminamos los artículos:\n",
        "    #Español\n",
        "    regex = re.compile(r\"\\b(el|la|los|las)\\b\", re.UNICODE)\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    #Inglés\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # Elimina los espacios en blanco\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "\n",
        "class ExactMatch(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Each `SquadExample` object contains the character level offsets for each token\n",
        "    in its input paragraph. We use them to get back the span of text corresponding\n",
        "    to the tokens between our predicted start and end tokens.\n",
        "    All the ground-truth answers are also present in each `SquadExample` object.\n",
        "    We calculate the percentage of data points where the span of text obtained\n",
        "    from model predictions matches one of the ground-truth answers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x_eval, y_eval):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "\n",
        "    # Esta función se ejecuta al término de cada época\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Obtiene las predicciones del modelo del conjunto de prueba\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "        count = 0\n",
        "        eval_examples_no_skip = [_ for _ in test if _.skip == False]\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "            squad_eg = eval_examples_no_skip[idx]\n",
        "            offsets = squad_eg.context_token_to_char\n",
        "            start = np.argmax(start)\n",
        "            end = np.argmax(end)\n",
        "            if start >= len(offsets):\n",
        "                continue\n",
        "            pred_char_start = offsets[start][0]\n",
        "            if end < len(offsets):\n",
        "                pred_char_end = offsets[end][1]\n",
        "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                pred_ans = squad_eg.context[pred_char_start:]\n",
        "\n",
        "            normalized_pred_ans = normalize_text(pred_ans)\n",
        "            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "            # Después de normalizar el texto, revisa si la predicción \n",
        "            # es un subconjunto de la respuesta real\n",
        "            if normalized_pred_ans in normalized_true_ans:\n",
        "                count += 1\n",
        "        acc = count / len(self.y_eval[0])\n",
        "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SJ4M4weBL25"
      },
      "source": [
        "##Entrenamiento & Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg9QS0dxBNyv"
      },
      "source": [
        "## Entrenamiento y evaluación del modelo \n",
        "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=3,  # de acuerdo con el paper, se recomienda trabajar con 3 épocas\n",
        "    verbose=2,\n",
        "    batch_size=64,\n",
        "    callbacks=[exact_match_callback],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_HExCHKjbEB"
      },
      "source": [
        "## Imprime los resultados del modelo \n",
        "print(exact_match_callback)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}